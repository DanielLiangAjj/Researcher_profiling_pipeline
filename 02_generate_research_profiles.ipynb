{
 "cells": [
  {
   "cell_type": "code",
   "id": "7f8466d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T22:39:30.846902Z",
     "start_time": "2025-11-20T22:39:30.836658Z"
    }
   },
   "source": "# Standard library imports\nimport os\nimport sys\nimport pickle\nimport time\nimport warnings\n\n# Data processing\nimport numpy as np\nimport pandas as pd\n\n# Machine learning\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom kneed import KneeLocator\n\n# Utilities\nfrom collections import defaultdict\nfrom tqdm import tqdm\n\n# OpenAI\nfrom openai import OpenAI\n\nwarnings.filterwarnings(\"ignore\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "11b8rqc1tzb",
   "source": "# Research Profile Generation Pipeline\n\nThis notebook processes MeSH terms from researcher publications and generates natural language research profiles using GPT-4.\n\n**Pipeline Steps:**\n1. Load MeSH tree hierarchy\n2. Process and expand MeSH terms to ancestors\n3. Filter low-frequency terms\n4. Categorize terms (Health Domain vs Methods)\n5. Remove meaningless terms\n6. Compute TF-IDF scores\n7. Select top terms using elbow detection\n8. Generate summaries with GPT-4\n9. Combine and export final profiles",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "3f31ecff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T22:39:30.860573Z",
     "start_time": "2025-11-20T22:39:30.858679Z"
    }
   },
   "source": "# =============================================================================\n# CONFIGURATION - Set your API key here\n# =============================================================================\nopenai_api_key = ''  # Enter your OpenAI API key",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c0lxcfjk7a7",
   "source": "## 1. Function Definitions\n\nThe following cells define all the helper functions used in the pipeline.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "7bbc3617",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T22:39:30.868691Z",
     "start_time": "2025-11-20T22:39:30.864557Z"
    }
   },
   "source": "\ndef load_mesh_trees(meshtree_file):\n    \"\"\"\n    Load a MeSH tree file and build ID-to-name and name-to-ID mappings.\n\n    Parameters\n    ----------\n    meshtree_file : str\n        Path to the MeSH tree file (e.g., \"mesh_tree_hierarchy.bin\").\n        Each line in the file is expected to be formatted as:\n            <term>;<tree_id>\n\n    Returns\n    -------\n    mesh_id2name : dict\n        Dictionary mapping MeSH tree IDs (e.g., \"A01.111\") to term names.\n    \n    mesh_name2id : dict of lists\n        Dictionary mapping MeSH term names to a list of associated MeSH IDs.\n\n    Notes\n    -----\n    The function also injects two additional MeSH-like IDs:\n        NEWID1 → Female  \n        NEWID2 → Male\n    \"\"\"\n    mesh_id2name = {}\n    mesh_name2id = defaultdict(list)\n\n\n    with open(meshtree_file, \"r\") as ftree:\n        for line in ftree:\n            term, tree_id = line.strip().split(\";\")\n\n            mesh_id2name[tree_id] = term\n            mesh_name2id[term].append(tree_id)\n\n    # Add the two extra synthetic entries\n    extra_entries = {\n        'NEWID1': 'Female',\n        'NEWID2': 'Male'\n    }\n\n    for mid, term in extra_entries.items():\n        mesh_id2name[mid] = term\n        mesh_name2id[term] = [mid]\n\n    return mesh_id2name, mesh_name2id",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "27b3b94d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T22:39:30.877706Z",
     "start_time": "2025-11-20T22:39:30.872319Z"
    }
   },
   "source": [
    "def process_mesh_terms(csv_file, mesh_id2name, mesh_name2id):\n",
    "    \"\"\"\n",
    "    Process researcher MeSH terms into hierarchical MeSH ancestor codes.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    csv_file : str\n",
    "        Path to the CSV file containing researcher MeSH terms.\n",
    "        Expected columns:\n",
    "            - First Name\n",
    "            - Last Name\n",
    "            - PMID\n",
    "            - Person ID\n",
    "            - mesh_term   (format: \"Term / Subheading\" or just \"Term\")\n",
    "\n",
    "    mesh_id2name : dict\n",
    "        Mapping from MeSH ID → MeSH term name.\n",
    "        Example: {\"A01.111\": \"Heart\"}.\n",
    "\n",
    "    mesh_name2id : dict or defaultdict(list)\n",
    "        Mapping from MeSH term name → list of associated MeSH IDs.\n",
    "        Example: {\"Heart\": [\"A01.111\", \"A01.112\"]}.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df_mesh_term : pandas.DataFrame\n",
    "        A cleaned dataframe with one row per:\n",
    "            (author × PMID × ancestor MeSH term)\n",
    "        Columns:\n",
    "            First Name, Last Name, PMID,\n",
    "            Person ID, ancestor_mesh_term\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    • Handles MeSH terms with and without subheadings.\n",
    "    • Automatically assigns:\n",
    "          Female → NEWID1\n",
    "          Male   → NEWID2\n",
    "    • Builds full hierarchical ancestor chains for every MeSH ID.\n",
    "    \"\"\"\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # Helper function: split mesh term into main + subheadings\n",
    "    # --------------------------------------------------\n",
    "    def split_mesh_term(mesh_term):\n",
    "        parts = mesh_term.split(\" / \")\n",
    "        if len(parts) > 1:\n",
    "            return parts[0], parts[1:]\n",
    "        return parts[0], []\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # Helper function: recursively find ancestors\n",
    "    # --------------------------------------------------\n",
    "    def find_mesh_term_ancestors(tid, ancestor_ids):\n",
    "        ancestors = []\n",
    "        mesh_name = mesh_id2name[tid]\n",
    "\n",
    "        # Add current term\n",
    "        ancestors.append(mesh_name)\n",
    "\n",
    "        # Get all IDs associated with the same MeSH term\n",
    "        ids_for_name = mesh_name2id[mesh_name]\n",
    "        ancestor_ids += ids_for_name\n",
    "\n",
    "        # Parent IDs = chop off last part of MeSH tree number\n",
    "        parent_ids = [\".\".join(mid.split(\".\")[:-1]) for mid in ids_for_name]\n",
    "\n",
    "        for pid in parent_ids:\n",
    "            if pid and pid not in ancestor_ids:\n",
    "                new_ancestors, ancestor_ids = find_mesh_term_ancestors(pid, ancestor_ids)\n",
    "                ancestors.extend(new_ancestors)\n",
    "                ancestors = list(set(ancestors))  # unique\n",
    "        return ancestors, ancestor_ids\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # Get only ancestor names\n",
    "    # --------------------------------------------------\n",
    "    def create_mesh_term_hierarchical_codes(tid):\n",
    "        ancestors, _ = find_mesh_term_ancestors(tid, [])\n",
    "        return ancestors\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # Load CSV\n",
    "    # --------------------------------------------------\n",
    "    df_mesh = pd.read_csv(csv_file)\n",
    "\n",
    "    # Split \"Term / Subheading\"\n",
    "    df_mesh['mesh_term_only'], df_mesh['mesh_subheading'] = zip(\n",
    "        *df_mesh['MeSH Term'].apply(split_mesh_term)\n",
    "    )\n",
    "\n",
    "    # Map main term → ID(s)\n",
    "    df_mesh['mesh_term_only_id'] = df_mesh['mesh_term_only'].apply(lambda x: mesh_name2id[x])\n",
    "\n",
    "    # Assign new synthetic IDs for Female / Male\n",
    "    df_mesh.loc[df_mesh['mesh_term_only'] == 'Female', 'mesh_term_only_id'] = 'NEWID1'\n",
    "    df_mesh.loc[df_mesh['mesh_term_only'] == 'Male',   'mesh_term_only_id'] = 'NEWID2'\n",
    "\n",
    "    # Expand list of IDs to one per row\n",
    "    df_mesh = df_mesh.explode(\"mesh_term_only_id\")\n",
    "\n",
    "    # Increase recursion depth (important)\n",
    "    sys.setrecursionlimit(5000)\n",
    "\n",
    "    # Compute full MeSH ancestor hierarchy\n",
    "    df_mesh['ancestor_mesh_term'] = df_mesh['mesh_term_only_id'].apply(\n",
    "        create_mesh_term_hierarchical_codes\n",
    "    )\n",
    "\n",
    "    # Expand ancestors to one per row\n",
    "    df_mesh = df_mesh.explode('ancestor_mesh_term')\n",
    "\n",
    "    # Select final output columns\n",
    "    df_mesh = df_mesh[\n",
    "        ['First Name', 'Last Name', 'PMID',\n",
    "         'Person ID', 'ancestor_mesh_term']\n",
    "    ]\n",
    "\n",
    "    # Remove duplicates\n",
    "    df_mesh.drop_duplicates(inplace=True)\n",
    "\n",
    "    return df_mesh\n"
   ],
   "outputs": [],
   "execution_count": 329
  },
  {
   "cell_type": "code",
   "id": "03b9a0cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T22:39:30.883972Z",
     "start_time": "2025-11-20T22:39:30.881570Z"
    }
   },
   "source": [
    "\n",
    "def filter_low_frequency_mesh_terms(df_mesh_term, min_frequency=2):\n",
    "    \"\"\"\n",
    "    Filter out MeSH ancestor terms that occur less than a specified frequency\n",
    "    for each researcher.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_mesh_term : pandas.DataFrame\n",
    "        DataFrame containing at least:\n",
    "            - Person ID\n",
    "            - ancestor_mesh_term\n",
    "\n",
    "    min_frequency : int, optional (default = 2)\n",
    "        Minimum number of occurrences required for a researcher to keep a term.\n",
    "        Terms occurring fewer times than this per Person ID are removed.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df_filtered : pandas.DataFrame\n",
    "        DataFrame with columns:\n",
    "            Person ID, ancestor_mesh_term, count\n",
    "        containing only the MeSH terms that meet the frequency threshold.\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute frequency of each ancestor term per researcher\n",
    "    df_freq = (\n",
    "        df_mesh_term\n",
    "        .groupby('Person ID')['ancestor_mesh_term']\n",
    "        .value_counts()\n",
    "        .rename('count')\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Keep terms at or above frequency threshold\n",
    "    df_filtered = df_freq[df_freq['count'] >= min_frequency]\n",
    "\n",
    "    return df_filtered\n"
   ],
   "outputs": [],
   "execution_count": 330
  },
  {
   "cell_type": "code",
   "id": "37419a45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T22:39:30.892064Z",
     "start_time": "2025-11-20T22:39:30.888363Z"
    }
   },
   "source": "\ndef categorize_mesh_terms(\n    df_mesh_term_freq,\n    mesh_id2name,\n    mesh_name2id,\n    class_file=\"mesh_category_classification.xlsx\"\n):\n    \"\"\"\n    Categorize MeSH ancestor terms into 'Health Domain' (H) and 'Method' (M)\n    classes using a manually tagged level-01 class file, then propagate\n    categories to child terms based on MeSH parent–child relationships.\n\n    Parameters\n    ----------\n    df_mesh_term_freq : pandas.DataFrame\n        Output of filter_low_frequency_mesh_terms(), must contain:\n            - ancestor_mesh_term\n\n    mesh_id2name : dict\n        Mapping MeSH ID -> MeSH term name\n\n    mesh_name2id : dict or defaultdict(list)\n        Mapping MeSH term name -> list of MeSH IDs\n\n    class_file : str\n        Excel file mapping MeSH terms to top-level class labels.\n        Must contain:\n            - 'name'  (MeSH term)\n            - 'Class' ('H' or 'M')\n\n    Returns\n    -------\n    mesh_names_health_domain : list\n        All MeSH term names categorized under Health Domain (H)\n\n    mesh_names_method : list\n        All MeSH term names categorized under Methods (M)\n    \"\"\"\n\n    # ------------------------------\n    # Load class definitions\n    # ------------------------------\n    df_class = pd.read_excel(class_file, index_col=0)\n\n    mesh_names_health_domain_manual = list(df_class[df_class['Class'] == 'H']['name'])\n    mesh_names_method_manual = list(df_class[df_class['Class'] == 'M']['name'])\n\n    # -------------------------------------------\n    # Get all unique ancestor terms from freq df\n    # -------------------------------------------\n    df_temp = df_mesh_term_freq[['ancestor_mesh_term']].drop_duplicates()\n\n    # Map term → IDs\n    df_temp['term_id'] = df_temp['ancestor_mesh_term'].apply(lambda x: mesh_name2id[x])\n    df_temp = df_temp.explode('term_id')\n\n    # Sort IDs by length so parents come first\n    df_temp['length'] = df_temp['term_id'].apply(len)\n    df_temp = df_temp.sort_values(by='length').drop_duplicates()\n\n    term_ids = list(df_temp['term_id'])\n\n    # -------------------------------------------\n    # Initialize category lists\n    # -------------------------------------------\n    mesh_names_health_domain = mesh_names_health_domain_manual.copy()\n    mesh_names_method = mesh_names_method_manual.copy()\n\n    # -------------------------------------------\n    # Categorize by tracing parent IDs\n    # -------------------------------------------\n    for tid in term_ids:\n        term_name = mesh_id2name[tid]\n\n        # Skip terms already categorized manually\n        if (term_name in mesh_names_health_domain_manual) or (term_name in mesh_names_method):\n            continue\n\n        parts = tid.split(\".\")\n        if len(parts) >= 2:\n            parent_id = \".\".join(parts[:-1])\n            parent_name = mesh_id2name[parent_id]\n\n            # Inherit health domain from parent\n            if parent_name in mesh_names_health_domain:\n                mesh_names_health_domain.append(term_name)\n\n            # Inherit method class from parent\n            if parent_name in mesh_names_method:\n                mesh_names_method.append(term_name)\n\n    # Remove duplicates\n    mesh_names_health_domain = list(set(mesh_names_health_domain))\n    mesh_names_method = list(set(mesh_names_method))\n\n    return mesh_names_health_domain, mesh_names_method",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "56f1f0b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T22:39:30.897780Z",
     "start_time": "2025-11-20T22:39:30.895040Z"
    }
   },
   "source": "# Terms that are too broad or non-informative to be useful\nMEANINGLESS_MESH_TERMS = [\n    'Eukaryota', 'Animals', 'Chordata', 'Vertebrates', 'Mammals', 'Eutheria',\n    'Primates', 'Haplorhini', 'Catarrhini', 'Hominidae', 'Humans',\n    'Natural Science Disciplines', 'Science', 'Research', 'Methods',\n    'Investigative Techniques', 'Persons', 'Health Occupations',\n    'Equipment and Supplies', 'Electrical Equipment and Supplies',\n    'Biomedical Research', 'Household Products', 'Photography',\n    'Financial Management', 'life', 'Medicine', 'Diseases'\n]\n\n\ndef remove_meaningless_mesh_terms(df_mesh_term_freq, mesh_id2name):\n    \"\"\"\n    Remove overly broad or non-informative MeSH terms.\n\n    Filters out:\n    - Predefined list of generic terms (e.g., \"Humans\", \"Animals\", \"Science\")\n    - Geographic terms (MeSH IDs starting with 'Z01')\n\n    Parameters\n    ----------\n    df_mesh_term_freq : pandas.DataFrame\n        DataFrame with ancestor_mesh_term column.\n    mesh_id2name : dict\n        Mapping from MeSH ID to term name.\n\n    Returns\n    -------\n    pandas.DataFrame\n        Filtered DataFrame with meaningless terms removed.\n    \"\"\"\n    # Start with predefined list\n    terms_to_remove = MEANINGLESS_MESH_TERMS.copy()\n    \n    # Add geographic terms (Z01.*)\n    for mesh_id, name in mesh_id2name.items():\n        if mesh_id.startswith('Z01'):\n            terms_to_remove.append(name)\n\n    return df_mesh_term_freq[\n        ~df_mesh_term_freq['ancestor_mesh_term'].isin(terms_to_remove)\n    ].copy()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "62dddd0f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T22:39:30.903489Z",
     "start_time": "2025-11-20T22:39:30.900923Z"
    }
   },
   "source": "def export_mesh_term_frequency_by_category(\n        df_mesh,\n        df_mesh_term_freq,\n        mesh_names_health_domain,\n        mesh_names_method,\n        directory\n    ):\n    \"\"\"\n    Export MeSH term frequency tables for Health Domain and Method categories.\n\n    Parameters\n    ----------\n    df_mesh : pandas.DataFrame\n        Must contain: First Name, Last Name, Person ID\n    df_mesh_term_freq : pandas.DataFrame\n        Must contain: Person ID, ancestor_mesh_term, count\n    mesh_names_health_domain : list\n        List of MeSH terms classified as Health Domain.\n    mesh_names_method : list\n        List of MeSH terms classified as Methods.\n    directory : str\n        Folder where Excel files will be saved.\n\n    Returns\n    -------\n    tuple\n        (health_domain_list, method_list) - Term frequencies as lists.\n    \"\"\"\n    df_person = df_mesh[['First Name', 'Last Name', 'Person ID']].drop_duplicates()\n\n    # Filter by category\n    df_health = df_mesh_term_freq[\n        df_mesh_term_freq['ancestor_mesh_term'].isin(mesh_names_health_domain)\n    ].merge(df_person, on='Person ID')\n\n    df_method = df_mesh_term_freq[\n        df_mesh_term_freq['ancestor_mesh_term'].isin(mesh_names_method)\n    ].merge(df_person, on='Person ID')\n\n    # Save to Excel\n    os.makedirs(directory, exist_ok=True)\n    df_health.to_excel(os.path.join(directory, \"mesh_term_freq_per_faculty_HealthDomain.xlsx\"), index=False)\n    df_method.to_excel(os.path.join(directory, \"mesh_term_freq_per_faculty_Method.xlsx\"), index=False)\n\n    return df_health.values.tolist(), df_method.values.tolist()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bdf142d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T22:39:30.908367Z",
     "start_time": "2025-11-20T22:39:30.906337Z"
    }
   },
   "source": "def build_researcher_mesh_string(mesh_term_freq_list):\n    \"\"\"\n    Build frequency-weighted term strings for TF-IDF input.\n    \n    Each term is repeated according to its frequency count.\n    Example: If \"Heart\" appears 3 times -> \"Heart;Heart;Heart\"\n\n    Parameters\n    ----------\n    mesh_term_freq_list : list of lists\n        Each row: [Person ID, term, freq, ...]\n\n    Returns\n    -------\n    dict\n        Mapping: Person ID -> \"term;term;term2;term2;...\"\n    \"\"\"\n    dict_researcher_mesh = defaultdict(str)\n\n    for row in mesh_term_freq_list:\n        person_id, term, freq = row[:3]\n        dict_researcher_mesh[person_id] += (term + \";\") * int(freq)\n\n    # Remove trailing semicolons\n    return {pid: terms.rstrip(\";\") for pid, terms in dict_researcher_mesh.items()}",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "725ddab8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T22:39:30.914981Z",
     "start_time": "2025-11-20T22:39:30.911300Z"
    }
   },
   "source": "def run_mesh_tfidf(dict_researcher_mesh, df_person, directory, postfix=\"HealthDomain\"):\n    \"\"\"\n    Compute TF-IDF scores for researcher MeSH term profiles.\n\n    Parameters\n    ----------\n    dict_researcher_mesh : dict\n        Mapping: Person ID -> \"term1;term1;term2;...\" (frequency-weighted)\n    df_person : pandas.DataFrame\n        Contains: Person ID, First Name, Last Name\n    directory : str\n        Output folder for CSV files.\n    postfix : str\n        Label for output files (\"HealthDomain\" or \"Method\").\n\n    Returns\n    -------\n    pandas.DataFrame\n        TF-IDF scores in long format (term, score, name).\n    \"\"\"\n    def custom_tokenizer(text):\n        return [token.strip() for token in text.split(';') if token.strip()]\n\n    corpus = list(dict_researcher_mesh.values())\n    faculty_ids = list(dict_researcher_mesh.keys())\n\n    # Compute TF-IDF\n    vectorizer = TfidfVectorizer(tokenizer=custom_tokenizer)\n    X = vectorizer.fit_transform(corpus)\n\n    # Convert to long-format DataFrame\n    df_tfidf = pd.DataFrame(X.toarray())\n    df_tfidf[\"Person ID\"] = faculty_ids\n    df_tfidf.set_index(\"Person ID\", inplace=True)\n\n    df_tfidf = df_tfidf.T\n    df_tfidf[\"mesh_term\"] = vectorizer.get_feature_names_out()\n    df_tfidf.set_index(\"mesh_term\", inplace=True)\n\n    df_tfidf = pd.DataFrame(df_tfidf.unstack()).reset_index()\n    df_tfidf.rename(columns={\"level_0\": \"Person ID\", 0: \"tfidf_score\"}, inplace=True)\n\n    # Add researcher names\n    df_tfidf = df_tfidf.merge(df_person, on=\"Person ID\")\n    df_tfidf[\"name\"] = df_tfidf[\"First Name\"] + \" \" + df_tfidf[\"Last Name\"]\n    df_tfidf.drop([\"Person ID\", \"First Name\", \"Last Name\"], axis=1, inplace=True)\n\n    # Save output\n    os.makedirs(directory, exist_ok=True)\n    df_tfidf.to_csv(os.path.join(directory, f\"term_per_researcher_tfidf_{postfix}.csv\"), index=False)\n\n    return df_tfidf",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cb294150",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T22:39:30.930346Z",
     "start_time": "2025-11-20T22:39:30.917656Z"
    }
   },
   "source": "def generate_gpt4_response(content, print_output=False):\n    \"\"\"\n    Send a prompt to GPT-4 and return the generated response.\n    \n    Parameters\n    ----------\n    content : str\n        The prompt to send to GPT-4.\n    print_output : bool\n        If True, print the full API response for debugging.\n    \n    Returns\n    -------\n    str or None\n        The generated text, or None if an error occurred.\n    \"\"\"\n    client = OpenAI(api_key=openai_api_key)\n    \n    try:\n        completions = client.chat.completions.create(\n            model=\"gpt-4o\",\n            temperature=0,  # Deterministic output\n            top_p=0.1,      # Focused sampling\n            n=1,\n            messages=[\n                {'role': 'system', 'content': 'You are a dean of a college.'},\n                {'role': 'user', 'content': content},\n            ]\n        )\n\n        if print_output:\n            print(completions)\n\n        return completions.choices[0].message.content\n    \n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return None",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6abdf015",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T22:39:30.937871Z",
     "start_time": "2025-11-20T22:39:30.933316Z"
    }
   },
   "source": "def generate_research_focus_summaries(postfix, directory, generate_gpt4_response, df_person, elbow_S=5):\n    \"\"\"\n    Generate research focus summaries for each researcher using GPT-4.\n\n    Uses TF-IDF scores to select top terms via elbow-point detection,\n    then sends them to GPT-4 for natural language summarization.\n\n    Parameters\n    ----------\n    postfix : str\n        Category label (\"Method\" or \"HealthDomain\").\n    directory : str\n        Folder for input/output files.\n    generate_gpt4_response : callable\n        Function to call GPT-4 API.\n    df_person : pandas.DataFrame\n        Researcher info (Person ID, First Name, Last Name).\n    elbow_S : int\n        Sensitivity parameter for KneeLocator (default: 5).\n\n    Returns\n    -------\n    tuple\n        (dict_terms, dict_summaries) - Selected terms and generated summaries.\n    \"\"\"\n    # Load TF-IDF scores\n    file_path = os.path.join(directory, f\"term_per_researcher_tfidf_{postfix}.csv\")\n    df_tfidf = pd.read_csv(file_path)\n    df_tfidf = df_tfidf[df_tfidf[\"tfidf_score\"] > 0]\n\n    # Build researcher -> {term: score} mapping\n    dict_mesh_tfidf = (\n        df_tfidf.groupby(\"name\")\n        .apply(lambda x: dict(zip(x[\"mesh_term\"], x[\"tfidf_score\"])))\n        .to_dict()\n    )\n\n    dict_terms = {}\n    dict_summaries = {}\n\n    for researcher_name in tqdm(dict_mesh_tfidf.keys()):\n        # Sort terms by TF-IDF score\n        df_temp = df_tfidf[df_tfidf[\"name\"] == researcher_name].copy()\n        df_temp.sort_values(\"tfidf_score\", ascending=False, inplace=True)\n        df_temp[\"rank\"] = np.arange(len(df_temp))\n\n        # Find elbow point for term selection\n        kneedle = KneeLocator(\n            df_temp[\"rank\"], df_temp[\"tfidf_score\"],\n            S=elbow_S, curve=\"convex\", direction=\"decreasing\"\n        )\n        \n        # Fallback: top 5% or at least 3 terms\n        knee_point = kneedle.knee if kneedle.knee else max(3, int(0.05 * len(df_temp)))\n        \n        # Select top terms\n        selected_terms = list(df_temp.iloc[:knee_point][\"mesh_term\"])\n        dict_terms[researcher_name] = \"; \".join(selected_terms)\n\n        # Generate GPT-4 summary\n        prompt = (\n            f\"Help me summarize this group of phrases into 1 sentence as a research focus:\\n\"\n            f\"{dict_terms[researcher_name]}\\n\"\n            f\"Please start with: The research focus is on\"\n        )\n\n        summary = None\n        while summary is None:\n            summary = generate_gpt4_response(prompt)\n            if summary is None:\n                print(f\"GPT-4 failed, retrying...\")\n                time.sleep(3)\n\n        # Adjust prefix based on category\n        if postfix == \"Method\":\n            summary = summary.replace(\"The research focus is on\", \"This researcher has mainly contributed to\")\n        else:\n            summary = summary.replace(\"The research focus is on\", \"This researcher mainly focused on\")\n\n        dict_summaries[researcher_name] = summary\n\n    # Save results\n    with open(os.path.join(directory, f\"dict_terms_for_a_researcher_for_focus{postfix}.pickle\"), \"wb\") as f:\n        pickle.dump(dict_terms, f, protocol=pickle.HIGHEST_PROTOCOL)\n    with open(os.path.join(directory, f\"dict_research_focus_for_a_researcher_{postfix}.pickle\"), \"wb\") as f:\n        pickle.dump(dict_summaries, f, protocol=pickle.HIGHEST_PROTOCOL)\n\n    return dict_terms, dict_summaries",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d76ff275",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T22:39:30.943677Z",
     "start_time": "2025-11-20T22:39:30.940703Z"
    }
   },
   "source": "def combine_research_focus_summaries(\n        directory,\n        postfix_hd=\"HealthDomain\",\n        postfix_m=\"Method\",\n        output_excel=\"Research_summary_byMesh.xlsx\"\n    ):\n    \"\"\"\n    Combine Health Domain and Method summaries into a single research profile.\n\n    Parameters\n    ----------\n    directory : str\n        Folder containing the pickle files.\n    postfix_hd : str\n        Postfix for Health Domain pickle file.\n    postfix_m : str\n        Postfix for Method pickle file.\n    output_excel : str\n        Output Excel filename.\n\n    Returns\n    -------\n    pandas.DataFrame\n        Combined profiles with columns: Researcher_name, Research_direction\n    \"\"\"\n    # Load Health Domain summaries\n    with open(os.path.join(directory, f\"dict_research_focus_for_a_researcher_{postfix_hd}.pickle\"), \"rb\") as f:\n        dict_hd = pickle.load(f)\n    df_hd = pd.DataFrame.from_dict(dict_hd, orient=\"index\").reset_index()\n    df_hd.columns = [\"Researcher_name\", \"Research_summary_hd\"]\n\n    # Load Method summaries\n    with open(os.path.join(directory, f\"dict_research_focus_for_a_researcher_{postfix_m}.pickle\"), \"rb\") as f:\n        dict_m = pickle.load(f)\n    df_m = pd.DataFrame.from_dict(dict_m, orient=\"index\").reset_index()\n    df_m.columns = [\"Researcher_name\", \"Research_summary_m\"]\n\n    # Merge and combine\n    df = df_hd.merge(df_m, on=\"Researcher_name\", how=\"outer\").fillna(\"\")\n    df[\"Research_direction\"] = (df[\"Research_summary_hd\"] + \"\\n\" + df[\"Research_summary_m\"]).str.strip(\"\\n\")\n\n    df_final = df[[\"Researcher_name\", \"Research_direction\"]]\n    df_final.to_excel(os.path.join(directory, output_excel), index=False)\n\n    return df_final",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d54ee96k8v",
   "source": "## 2. Pipeline Execution\n\nThe following cells execute the pipeline step by step.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "038e0883",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T22:39:30.947664Z",
     "start_time": "2025-11-20T22:39:30.946360Z"
    }
   },
   "source": "# =============================================================================\n# FILE PATHS\n# =============================================================================\ndirectory = \"results/intermediate_result\"\ninput_file = \"data/output_sample/preprocess/filtered_publications.csv\"",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "19290c23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T22:39:30.994928Z",
     "start_time": "2025-11-20T22:39:30.950166Z"
    }
   },
   "source": "# Step 1: Load MeSH tree hierarchy\nmeshtree_file = \"mesh_tree_hierarchy.bin\"\nmesh_id2name, mesh_name2id = load_mesh_trees(meshtree_file)\nprint(f\"Loaded {len(mesh_id2name)} MeSH terms\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "04e18282",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T22:39:31.063975Z",
     "start_time": "2025-11-20T22:39:31.001451Z"
    }
   },
   "source": "# Step 2: Process MeSH terms and expand to ancestors\ndf_mesh_term = process_mesh_terms(input_file, mesh_id2name, mesh_name2id)\nprint(f\"Processed {len(df_mesh_term)} term-publication pairs\")\ndf_mesh_term.head()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c459d83f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T22:39:31.095823Z",
     "start_time": "2025-11-20T22:39:31.090130Z"
    }
   },
   "source": "# Step 3: Filter low-frequency terms (keep terms appearing 2+ times)\ndf_mesh_term_freq = filter_low_frequency_mesh_terms(df_mesh_term, min_frequency=2)\nprint(f\"Kept {len(df_mesh_term_freq)} term-researcher pairs after frequency filtering\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e99e0b11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T22:39:31.194350Z",
     "start_time": "2025-11-20T22:39:31.127229Z"
    }
   },
   "source": "# Step 4: Categorize terms into Health Domain vs Methods\nmesh_names_health_domain, mesh_names_method = categorize_mesh_terms(\n    df_mesh_term_freq,\n    mesh_id2name,\n    mesh_name2id,\n    class_file=\"mesh_category_classification.xlsx\"\n)\nprint(f\"Health Domain terms: {len(mesh_names_health_domain)}\")\nprint(f\"Method terms: {len(mesh_names_method)}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b4eb2185",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T22:39:31.207008Z",
     "start_time": "2025-11-20T22:39:31.199693Z"
    }
   },
   "source": "# Step 5: Remove meaningless/overly broad terms\ndf_mesh_term_freq = remove_meaningless_mesh_terms(df_mesh_term_freq, mesh_id2name)\nprint(f\"Remaining terms after removing meaningless: {len(df_mesh_term_freq)}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "aa3fb96e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T22:39:31.243354Z",
     "start_time": "2025-11-20T22:39:31.210687Z"
    }
   },
   "source": "# Step 6: Export term frequencies by category\nmesh_term_freq_list_health_domain, mesh_term_freq_list_method = export_mesh_term_frequency_by_category(\n    df_mesh_term,\n    df_mesh_term_freq,\n    mesh_names_health_domain,\n    mesh_names_method,\n    directory=directory\n)\nprint(f\"Exported {len(mesh_term_freq_list_health_domain)} health domain entries\")\nprint(f\"Exported {len(mesh_term_freq_list_method)} method entries\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9c1649bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T22:39:31.253542Z",
     "start_time": "2025-11-20T22:39:31.247695Z"
    }
   },
   "source": "# Step 7: Build frequency-weighted term strings for TF-IDF\ndict_researcher_mesh_health_domain = build_researcher_mesh_string(mesh_term_freq_list_health_domain)\ndict_researcher_mesh_method = build_researcher_mesh_string(mesh_term_freq_list_method)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "753edd5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T22:39:31.328441Z",
     "start_time": "2025-11-20T22:39:31.257568Z"
    }
   },
   "source": "# Step 8: Compute TF-IDF scores\ndf_person = df_mesh_term[['First Name', 'Last Name', 'Person ID']].drop_duplicates()\n\nprint(\"Computing TF-IDF for Health Domain terms...\")\nrun_mesh_tfidf(\n    dict_researcher_mesh=dict_researcher_mesh_health_domain,\n    df_person=df_person,\n    directory=directory,\n    postfix=\"HealthDomain\"\n)\n\nprint(\"Computing TF-IDF for Method terms...\")\nrun_mesh_tfidf(\n    dict_researcher_mesh=dict_researcher_mesh_method,\n    df_person=df_person,\n    directory=directory,\n    postfix=\"Method\"\n)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a1d90f2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T22:39:35.769762Z",
     "start_time": "2025-11-20T22:39:31.385840Z"
    }
   },
   "source": "# Step 9: Generate research focus summaries using GPT-4\nprint(\"Generating Method summaries...\")\ndict_terms_method, dict_focus_method = generate_research_focus_summaries(\n    postfix=\"Method\",\n    directory=directory,\n    generate_gpt4_response=generate_gpt4_response,\n    df_person=df_person\n)\n\nprint(\"\\nGenerating Health Domain summaries...\")\ndict_terms_hd, dict_focus_hd = generate_research_focus_summaries(\n    postfix=\"HealthDomain\",\n    directory=directory,\n    generate_gpt4_response=generate_gpt4_response,\n    df_person=df_person\n)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "45d37c6f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T22:39:35.850595Z",
     "start_time": "2025-11-20T22:39:35.840313Z"
    }
   },
   "source": "# Step 10: Combine summaries and export final results\ndf_summary = combine_research_focus_summaries(\n    directory=directory,\n    postfix_hd=\"HealthDomain\",\n    postfix_m=\"Method\",\n    output_excel=\"Research_summary_byMesh.xlsx\"\n)\n\nprint(f\"Generated profiles for {len(df_summary)} researchers\")\nprint(f\"Output saved to: {directory}/Research_summary_byMesh.xlsx\")\ndf_summary",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}